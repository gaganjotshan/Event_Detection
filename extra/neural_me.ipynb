{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"neural_me.ipynb","provenance":[],"mount_file_id":"10Xk6Er3gVVrivz082odoAcAnS39K0F3X","authorship_tag":"ABX9TyNopZoHUyNLZUGtIImtCYpH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4fzKgBtORD1W"},"outputs":[],"source":["import keras\n","import numpy as np\n","from sklearn import preprocessing\n","from keras.layers import Dense, Input, Embedding, Lambda, Layer, Multiply, Dropout, Dot\n","from keras.models import Model, Sequential\n","from sklearn.feature_extraction.text import CountVectorizer\n","from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot\n","from keras import backend as K\n","import tensorflow as tf\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n","import pandas as pd"]},{"cell_type":"code","source":["import logging, os\n","logging.disable(logging.WARNING)\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n","import tensorflow as tf"],"metadata":{"id":"DxpUNDOtAgic"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_embeddings(filename, max_vocab_size):\n","\n","    vocab={}\n","    embeddings=[]\n","    with open(filename) as file:\n","        \n","        cols=file.readline().split(\" \")\n","        num_words=int(cols[0])\n","        size=int(cols[1])\n","        embeddings.append(np.zeros(size))  # 0 = 0 padding if needed\n","        embeddings.append(np.zeros(size))  # 1 = UNK\n","        vocab[\"_0_\"]=0\n","        vocab[\"_UNK_\"]=1\n","        \n","        for idx,line in enumerate(file):\n","\n","            if idx+2 >= max_vocab_size:\n","                break\n","\n","            cols=line.rstrip().split(\" \")\n","            val=np.array(cols[1:])\n","            word=cols[0]\n","            \n","            embeddings.append(val)\n","            vocab[word]=idx+2\n","\n","    return np.array(embeddings), vocab, size"],"metadata":{"id":"jW-BFgWESFRC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_data(filename, vocab):\n","    X=[]\n","    Y=[]\n","    with open(filename, encoding=\"utf-8\") as file:\n","        for line in file:\n","            cols=line.rstrip().split(\"\\t\")\n","            label=cols[1]\n","            # assumes text is already tokenized\n","            text=cols[1].split(\" \")\n","            X.append(text)\n","            Y.append(label)\n","    return X, Y"],"metadata":{"id":"bSdfJJ7YSK7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_word_ids(docs, vocab, max_length=1000):\n","    \n","    doc_ids=[]\n","    \n","    for doc in docs:\n","        wids=[]\n","\n","        for token in doc[:max_length]:\n","            val = vocab[token.lower()] if token.lower() in vocab else 1\n","            wids.append(val)\n","        \n","        # pad each document to constant width\n","        for i in range(len(wids),max_length):\n","            wids.append(0)\n","\n","        doc_ids.append(wids)\n","\n","    return np.array(doc_ids)"],"metadata":{"id":"kz4RJxRxSOot"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embeddings, vocab, embedding_size=load_embeddings(\"/content/drive/MyDrive/Event_Extraction/embeddings/embeddings.txt\", 10000)"],"metadata":{"id":"NApV4lonSTlX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["directory=\"/content/drive/MyDrive/Event_Extraction/files\""],"metadata":{"id":"5_b10BNKleiN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainText, trainY = read_data(\"%s/train_answer_keys.txt\" % directory, vocab)\n","devText, devY=read_data(\"%s/val_answer_keys.txt\" % directory, vocab)"],"metadata":{"id":"q89sRqzUqTZz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainX = get_word_ids(trainText, vocab, max_length=200)\n","devX = get_word_ids(devText, vocab, max_length=200)"],"metadata":{"id":"MXakY2VZtNUo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["le = preprocessing.LabelEncoder()\n","le.fit(trainY)\n","Y_train=np.array(le.transform(trainY))\n","Y_dev=np.array(le.transform(devY))"],"metadata":{"id":"-v6Mkk2StXHt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["First, let's try a simple model that represents a document by averaging the embeddings for the words it contains. We'll again use appropriate masking to accommodate zero-padded sequences."],"metadata":{"id":"OcEGVak5vFUT"}},{"cell_type":"code","source":["from keras import layers\n","class MaskedAveragePooling1D(Layer):\n","    def __init__(self, **kwargs):\n","        self.supports_masking = True\n","        super(MaskedAveragePooling1D, self).__init__(**kwargs)\n","\n","    def compute_mask(self, input, input_mask=None):\n","        return None\n","\n","    def call(self, x, mask=None):\n","        if mask is not None:\n","            mask = K.cast(mask, K.floatx())\n","            mask = K.repeat(mask, x.shape[-1])\n","            mask = tf.transpose(mask, [0,2,1])\n","            # zero out the elements of x that are masked\n","            x = x * mask\n","            \n","        # sum the modified input, but normalize only over the number of non-masked time steps\n","        return K.sum(x, axis=1) / K.sum(mask, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], input_shape[2])"],"metadata":{"id":"O1cDdAlMteeQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_embedding_average(embeddings):\n","\n","    vocab_size, word_embedding_dim=embeddings.shape\n","    \n","    word_sequence_input = Input(shape=(None,), dtype='int32')\n","    \n","    word_embedding_layer = Embedding(vocab_size,\n","                                    word_embedding_dim,\n","                                    weights=[embeddings],\n","                                    mask_zero=True,\n","                                    trainable=False)\n","\n","    \n","    embedded_sequences = word_embedding_layer(word_sequence_input)\n","    x=MaskedAveragePooling1D()(embedded_sequences)\n","    \n","    predictions=Dense(1, activation=\"sigmoid\")(x)\n","\n","    model = Model(inputs=word_sequence_input, outputs=predictions)\n","\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n","    \n","    return model"],"metadata":{"id":"-C2Lr-buwJOl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_model=get_embedding_average(embeddings)\n","print (embedding_model.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Krs8CEbwNHj","executionInfo":{"status":"ok","timestamp":1653376366512,"user_tz":-120,"elapsed":1634,"user":{"displayName":"Gaganjot Shan","userId":"18300755219130239353"}},"outputId":"b2ee350d-e7bc-4b65-e8c0-59ca5e082252"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," embedding (Embedding)       (None, None, 300)         3000000   \n","                                                                 \n"," masked_average_pooling1d (M  (None, 300)              0         \n"," askedAveragePooling1D)                                          \n","                                                                 \n"," dense (Dense)               (None, 1)                 301       \n","                                                                 \n","=================================================================\n","Total params: 3,000,301\n","Trainable params: 301\n","Non-trainable params: 3,000,000\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","source":["model=embedding_model\n","\n","modelName=\"embedding_model.hdf5\"\n","checkpoint = ModelCheckpoint(modelName, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n","\n","model.fit(trainX, Y_train, \n","            validation_data=(devX, Y_dev),\n","            epochs=30, batch_size=128,\n","            callbacks=[checkpoint])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IYxKHcyTwceE","executionInfo":{"status":"ok","timestamp":1653376496692,"user_tz":-120,"elapsed":130184,"user":{"displayName":"Gaganjot Shan","userId":"18300755219130239353"}},"outputId":"911033af-dbf8-48cc-f0d6-b4ff0b1cbbbc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","57/57 [==============================] - 5s 75ms/step - loss: -1.1745 - acc: 0.0813 - val_loss: -3.2315 - val_acc: 0.0821\n","Epoch 2/30\n"," 2/57 [>.............................] - ETA: 3s - loss: -3.4960 - acc: 0.1055"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  layer_config = serialize_layer_fn(layer)\n"]},{"output_type":"stream","name":"stdout","text":["57/57 [==============================] - 5s 87ms/step - loss: -5.0965 - acc: 0.0824 - val_loss: -7.1616 - val_acc: 0.0821\n","Epoch 3/30\n","57/57 [==============================] - 5s 80ms/step - loss: -8.9577 - acc: 0.0824 - val_loss: -11.0480 - val_acc: 0.0821\n","Epoch 4/30\n","57/57 [==============================] - 4s 76ms/step - loss: -12.8100 - acc: 0.0824 - val_loss: -14.9509 - val_acc: 0.0821\n","Epoch 5/30\n","57/57 [==============================] - 4s 77ms/step - loss: -16.6567 - acc: 0.0824 - val_loss: -18.8356 - val_acc: 0.0821\n","Epoch 6/30\n","57/57 [==============================] - 4s 77ms/step - loss: -20.5147 - acc: 0.0824 - val_loss: -22.7533 - val_acc: 0.0821\n","Epoch 7/30\n","57/57 [==============================] - 4s 77ms/step - loss: -24.3864 - acc: 0.0824 - val_loss: -26.6526 - val_acc: 0.0821\n","Epoch 8/30\n","57/57 [==============================] - 4s 75ms/step - loss: -28.2651 - acc: 0.0824 - val_loss: -30.5638 - val_acc: 0.0821\n","Epoch 9/30\n","57/57 [==============================] - 6s 113ms/step - loss: -32.1430 - acc: 0.0824 - val_loss: -34.4751 - val_acc: 0.0821\n","Epoch 10/30\n","57/57 [==============================] - 4s 77ms/step - loss: -36.0269 - acc: 0.0824 - val_loss: -38.4411 - val_acc: 0.0821\n","Epoch 11/30\n","57/57 [==============================] - 4s 76ms/step - loss: -39.9037 - acc: 0.0824 - val_loss: -42.2962 - val_acc: 0.0821\n","Epoch 12/30\n","57/57 [==============================] - 4s 71ms/step - loss: -43.7750 - acc: 0.0824 - val_loss: -46.2429 - val_acc: 0.0821\n","Epoch 13/30\n","57/57 [==============================] - 4s 75ms/step - loss: -47.6585 - acc: 0.0824 - val_loss: -50.1514 - val_acc: 0.0821\n","Epoch 14/30\n","57/57 [==============================] - 4s 77ms/step - loss: -51.5221 - acc: 0.0824 - val_loss: -54.0413 - val_acc: 0.0821\n","Epoch 15/30\n","57/57 [==============================] - 4s 68ms/step - loss: -55.3779 - acc: 0.0824 - val_loss: -57.9868 - val_acc: 0.0821\n","Epoch 16/30\n","57/57 [==============================] - 4s 70ms/step - loss: -59.2504 - acc: 0.0824 - val_loss: -61.8490 - val_acc: 0.0821\n","Epoch 17/30\n","57/57 [==============================] - 4s 77ms/step - loss: -63.1100 - acc: 0.0824 - val_loss: -65.7554 - val_acc: 0.0821\n","Epoch 18/30\n","57/57 [==============================] - 4s 77ms/step - loss: -66.9776 - acc: 0.0824 - val_loss: -69.6697 - val_acc: 0.0821\n","Epoch 19/30\n","57/57 [==============================] - 4s 77ms/step - loss: -70.8490 - acc: 0.0824 - val_loss: -73.5782 - val_acc: 0.0821\n","Epoch 20/30\n","57/57 [==============================] - 4s 75ms/step - loss: -74.7191 - acc: 0.0824 - val_loss: -77.4647 - val_acc: 0.0821\n","Epoch 21/30\n","57/57 [==============================] - 4s 76ms/step - loss: -78.5677 - acc: 0.0824 - val_loss: -81.3911 - val_acc: 0.0821\n","Epoch 22/30\n","57/57 [==============================] - 4s 68ms/step - loss: -82.4510 - acc: 0.0824 - val_loss: -85.2874 - val_acc: 0.0821\n","Epoch 23/30\n","57/57 [==============================] - 4s 65ms/step - loss: -86.2991 - acc: 0.0824 - val_loss: -89.1824 - val_acc: 0.0821\n","Epoch 24/30\n","57/57 [==============================] - 3s 61ms/step - loss: -90.1718 - acc: 0.0824 - val_loss: -93.0805 - val_acc: 0.0821\n","Epoch 25/30\n","57/57 [==============================] - 3s 61ms/step - loss: -94.0362 - acc: 0.0824 - val_loss: -97.0001 - val_acc: 0.0821\n","Epoch 26/30\n","57/57 [==============================] - 4s 77ms/step - loss: -97.9380 - acc: 0.0824 - val_loss: -100.8964 - val_acc: 0.0821\n","Epoch 27/30\n","57/57 [==============================] - 4s 66ms/step - loss: -101.7980 - acc: 0.0824 - val_loss: -104.8348 - val_acc: 0.0821\n","Epoch 28/30\n","57/57 [==============================] - 4s 77ms/step - loss: -105.6798 - acc: 0.0824 - val_loss: -108.7412 - val_acc: 0.0821\n","Epoch 29/30\n","57/57 [==============================] - 5s 83ms/step - loss: -109.5388 - acc: 0.0824 - val_loss: -112.6550 - val_acc: 0.0821\n","Epoch 30/30\n","57/57 [==============================] - 4s 76ms/step - loss: -113.4088 - acc: 0.0824 - val_loss: -116.5471 - val_acc: 0.0821\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f4b9ae46890>"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["class AttentionLayerMasking(Layer):\n","\n","    def __init__(self, output_dim, **kwargs):\n","        self.output_dim = output_dim\n","        super(AttentionLayerMasking, self).__init__(**kwargs)\n","\n","\n","    def build(self, input_shape):\n","        input_embedding_dim=input_shape[-1]\n","        \n","        self.kernel = self.add_weight(name='kernel', \n","                            shape=(input_embedding_dim,1),\n","                            initializer='uniform',\n","                            trainable=True)\n","        super(AttentionLayerMasking, self).build(input_shape)\n","\n","    def compute_mask(self, input, input_mask=None):\n","        return None\n","\n","    def call(self, x, mask=None):\n","        \n","        # dot product \n","        x=K.dot(x, self.kernel)\n","        # exponentiate\n","        x=K.exp(x)\n","        \n","        # zero out elements that are masked\n","        if mask is not None:\n","            mask = K.cast(mask, K.floatx())\n","            mask = K.expand_dims(mask, axis=-1)\n","            x = x * mask\n","        \n","        # normalize by sum\n","        x /= K.sum(x, axis=1, keepdims=True)\n","        x=K.squeeze(x, axis=2)\n","\n","        return x\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], input_shape[1])"],"metadata":{"id":"kJWaZdjjwvri"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_embedding_with_attention_masking(embeddings):\n","\n","    vocab_size, word_embedding_dim=embeddings.shape\n","    \n","    word_sequence_input = Input(shape=(None,), dtype='int32')\n","    \n","    word_embedding_layer = Embedding(vocab_size,\n","                                    word_embedding_dim,\n","                                    weights=[embeddings], \n","                                    mask_zero=True,\n","                                    trainable=False)\n","\n","    \n","    embedded_sequences = word_embedding_layer(word_sequence_input)\n","    \n","    #transform each word embedding into a new vector to use for measuring its importance\n","    attention_key_dim=300\n","    attention_input=Dense(attention_key_dim, activation='tanh')(embedded_sequences)\n","\n","    # pass those transformed inputs through an attention layer, getting back a normalized\n","    # attention value a_i for each token i; \\forall i, 0 <= a_i <= 1; for a document with N words, \n","    # \\sum_{i=0}^N a_i = 1\n","    \n","    attention_output = AttentionLayerMasking(word_embedding_dim, name=\"attention\")(attention_input)\n","    \n","    # now let's multiply those attention weights by original inputs to get a weighted average over them\n","    document_representation = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=1), name='dot')([attention_output,embedded_sequences])\n","\n","    x=Dense(1, activation=\"sigmoid\")(document_representation)\n","\n","    model = Model(inputs=word_sequence_input, outputs=x)\n","\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n","    \n","    return model\n"],"metadata":{"id":"ICVOFRYdxXNq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_attention_model = get_embedding_with_attention_masking(embeddings)\n","print (embedding_attention_model.summary())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RP74PQ4jxlVC","executionInfo":{"status":"ok","timestamp":1653376498550,"user_tz":-120,"elapsed":1540,"user":{"displayName":"Gaganjot Shan","userId":"18300755219130239353"}},"outputId":"86a39cef-b378-4e00-dfe4-160ba96e44d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_2 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," embedding_1 (Embedding)        (None, None, 300)    3000000     ['input_2[0][0]']                \n","                                                                                                  \n"," dense_1 (Dense)                (None, None, 300)    90300       ['embedding_1[0][0]']            \n","                                                                                                  \n"," attention (AttentionLayerMaski  (None, None)        300         ['dense_1[0][0]']                \n"," ng)                                                                                              \n","                                                                                                  \n"," dot (Lambda)                   (None, 300)          0           ['attention[0][0]',              \n","                                                                  'embedding_1[0][0]']            \n","                                                                                                  \n"," dense_2 (Dense)                (None, 1)            301         ['dot[0][0]']                    \n","                                                                                                  \n","==================================================================================================\n","Total params: 3,090,901\n","Trainable params: 90,901\n","Non-trainable params: 3,000,000\n","__________________________________________________________________________________________________\n","None\n"]}]},{"cell_type":"code","source":["model = embedding_attention_model\n","\n","modelName=\"embedding_attention_model.hdf5\"\n","checkpoint = ModelCheckpoint(modelName, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n","\n","model.fit(trainX, Y_train, \n","            validation_data=(devX, Y_dev),\n","            epochs=30, batch_size=128,\n","            callbacks=[checkpoint])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":802},"id":"tH2oAMp2xpdn","executionInfo":{"status":"error","timestamp":1653376513462,"user_tz":-120,"elapsed":14915,"user":{"displayName":"Gaganjot Shan","userId":"18300755219130239353"}},"outputId":"e412ce07-bf44-4f8b-d4d2-67586db3f92a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","57/57 [==============================] - ETA: 0s - loss: -1.2559 - acc: 0.0820"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  layer_config = serialize_layer_fn(layer)\n"]},{"output_type":"error","ename":"NotImplementedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-5265cc6409ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             callbacks=[checkpoint])\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m                       \u001b[0;34m\"arg2\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                   }})\n\u001b[0;32m--> 776\u001b[0;31m                   return config\"\"\"))\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: \nLayer AttentionLayerMasking has arguments ['self', 'output_dim']\nin `__init__` and therefore must override `get_config()`.\n\nExample:\n\nclass CustomLayer(keras.layers.Layer):\n    def __init__(self, arg1, arg2):\n        super().__init__()\n        self.arg1 = arg1\n        self.arg2 = arg2\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"arg1\": self.arg1,\n            \"arg2\": self.arg2,\n        })\n        return config"]}]},{"cell_type":"code","source":["# load the best saved model\n","\n","model=embedding_attention_model\n","model.load_weights(\"embedding_attention_model.hdf5\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"RZNnxb7TxtD0","executionInfo":{"status":"error","timestamp":1653376631529,"user_tz":-120,"elapsed":213,"user":{"displayName":"Gaganjot Shan","userId":"18300755219130239353"}},"outputId":"36e288c4-0aa0-4d85-a0e9-8dc26c3d2615"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-182cf790ae3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_attention_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embedding_attention_model.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, model)\u001b[0m\n\u001b[1;32m    727\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     raise ValueError(\n\u001b[0;32m--> 729\u001b[0;31m         \u001b[0;34mf'Layer count mismatch when loading weights from file. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;34mf'Model expected {len(filtered_layers)} layers, found '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         f'{len(layer_names)} saved layers.')\n","\u001b[0;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 4 layers, found 0 saved layers."]}]},{"cell_type":"markdown","source":["adding attention to that simple model to learn a weighted average over words---giving more weight to words in the document that are more important for representing the document for the purpose of this classification"],"metadata":{"id":"UL7-b4KP6Mah"}},{"cell_type":"code","source":["def analyze(model, doc):\n","    \n","    words=doc.split(\" \")\n","    text = get_word_ids([words], vocab, max_length=len(words))\n","   \n","    inp = model.input                                    \n","    outputs = [layer.output for layer in model.layers[1:]]       \n","    functor = K.function([inp, K.learning_phase()], outputs) \n","\n","    test = text[0]\n","    orig=words\n","    attention_weights=[]\n","    test=test.reshape((1,len(words)))\n","    layer_outs = functor([test, 0.])\n","\n","    # in this model, attention is the third layer\n","    attention_layer=layer_outs[2]\n","    \n","    for i in range(len(words)):\n","        val=attention_layer[0,i]\n","        attention_weights.append(val)\n","        print (\"%.3f\\t%s\" % (val, orig[i]))\n","        \n","    df = pd.DataFrame({'words':orig, 'attention':attention_weights})\n","    ax = df.plot.bar(x='words', y='attention', figsize=(10,4))"],"metadata":{"id":"bPT7leeaxyfQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"m3_hpSThzKr1"}}]}